{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch version: 1.10.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from gymnasium.wrappers.pixel_observation import PixelObservationWrapper\n",
    "import argparse\n",
    "import os\n",
    "from env_utils import *\n",
    "from Model.Actor import Policy\n",
    "from Model.Critic import StateValue\n",
    "from utils import OrnsteinUhlenbeckActionNoise, replayBuffer, subplot, obs_processing\n",
    "\n",
    "from torch import nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as opt \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn #needed for building neural networks\n",
    "import torch.nn.functional as F #needed for activation functions\n",
    "from env_utils import *\n",
    "import torch.optim as opt #needed for optimisation\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "print(\"Using torch version: {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE=1000000\n",
    "BATCH_SIZE=64\n",
    "GAMMA=0.99\n",
    "TAU=0.001       #Target Network HyperParameters Update rate\n",
    "LRA=0.0001      #LEARNING RATE ACTOR\n",
    "LRC=0.001       #LEARNING RATE CRITIC\n",
    "H1=400   #neurons of 1st layers\n",
    "H2=300   #neurons of 2nd layers\n",
    "\n",
    "MAX_EPISODES=5000 #number of episodes of the training\n",
    "MAX_STEPS=200    #max steps to finish an episode. An episode breaks early if some break conditions are met (like too much\n",
    "                  #amplitude of the joints angles or if a failure occurs). In the case of pendulum there is no break \n",
    "                #condition, hence no environment reset,  so we just put 1 step per episode. \n",
    "buffer_start = 100 #initial warmup without training\n",
    "epsilon = 1\n",
    "epsilon_decay = 1./100000 #this is ok for a simple task like inverted pendulum, but maybe this would be set to zero for more\n",
    "                     #complex tasks like Hopper; epsilon is a decay for the exploration and noise applied to the action is \n",
    "                     #weighted by this decay. In more complex tasks we need the exploration to not vanish so we set the decay\n",
    "                     #to zero.\n",
    "PRINT_EVERY = 10 #Print info about average reward every PRINT_EVERY\n",
    "\n",
    "ENV_NAME = \"CustomHopper-source-v0\" # Put here the gym env name you want to play with\n",
    "#check other environments to play with at https://gym.openai.com/envs/#mujoco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer\n",
    "It would be interesting to use prioritise experience replay. Have you ever managed to use the prioritised experience replay with DDPG? Leave a comment if you would like to share your results with prioritised experience replay. https://arxiv.org/pdf/1511.05952.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replayBuffer(object):\n",
    "    def __init__(self, buffer_size, name_buffer=''):\n",
    "        self.buffer_size=buffer_size  #choose buffer size\n",
    "        self.num_exp=0\n",
    "        self.buffer=deque()\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience=(s, a, r, t, s2)\n",
    "        if self.num_exp < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.num_exp +=1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def count(self):\n",
    "        return self.num_exp\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.num_exp < batch_size:\n",
    "            batch=random.sample(self.buffer, self.num_exp)\n",
    "        else:\n",
    "            batch=random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s, a, r, t, s2 = map(np.stack, zip(*batch))\n",
    "\n",
    "        return s, a, r, t, s2\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_exp=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job will run on cuda\n"
     ]
    }
   ],
   "source": [
    "#set GPU for faster training\n",
    "cuda = torch.cuda.is_available() #check for CUDA\n",
    "device   = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "print(\"Job will run on {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architectures\n",
    "We define here the networks. Note that better results in complex tasks (like Hopper env) have to be addressed using higher batch size (so 128 instead of 64) and with batch normalisation layers in between input and hidden layers of both actoir and critic; so uncomment the lines referring to BN to face more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanin_(size):\n",
    "    fan_in = size[0]\n",
    "    weight = 1./np.sqrt(fan_in)\n",
    "    return torch.Tensor(size).uniform_(-weight, weight)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, h1=H1, h2=H2, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "                \n",
    "        self.linear1 = nn.Linear(state_dim, h1)\n",
    "        self.linear1.weight.data = fanin_(self.linear1.weight.data.size())\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(h1)\n",
    "        \n",
    "        self.linear2 = nn.Linear(h1+action_dim, h2)\n",
    "        self.linear2.weight.data = fanin_(self.linear2.weight.data.size())\n",
    "                \n",
    "        self.linear3 = nn.Linear(h2, 1)\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = self.linear1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(torch.cat([x,action],1))\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class Actor(nn.Module): \n",
    "    def __init__(self, state_dim, action_dim, h1=H1, h2=H2, init_w=0.003):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        #self.bn0 = nn.BatchNorm1d(state_dim)\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, h1)\n",
    "        self.linear1.weight.data = fanin_(self.linear1.weight.data.size())\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(h1)\n",
    "        \n",
    "        self.linear2 = nn.Linear(h1, h2)\n",
    "        self.linear2.weight.data = fanin_(self.linear2.weight.data.size())\n",
    "        \n",
    "        #self.bn2 = nn.BatchNorm1d(h2)\n",
    "        \n",
    "        self.linear3 = nn.Linear(h2, action_dim)\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, state):\n",
    "        #state = self.bn0(state)\n",
    "        x = self.linear1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state  = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.forward(state)\n",
    "        return action.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "As described in the paper, we have to add noise to the action in order to ensure exploration. An Ornstein-Uhlenbeck process is chosen because it adds noise in a smooth way, which is suitable for continuous control tasks. More details on this random process are simply described on Wikipedia: https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu=0, sigma=0.2, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap state and action spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def _action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim: 11, Action dim: 3\n"
     ]
    }
   ],
   "source": [
    "env = make_env(domain=\"source\",  render_mode='rgb_array')\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"State dim: {}, Action dim: {}\".format(state_dim, action_dim))\n",
    "\n",
    "noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "critic  = Critic(state_dim, action_dim).to(device)\n",
    "actor = Actor(state_dim, action_dim).to(device)\n",
    "\n",
    "target_critic  = Critic(state_dim, action_dim).to(device)\n",
    "target_actor = Actor(state_dim, action_dim).to(device)\n",
    "\n",
    "for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "q_optimizer  = opt.Adam(critic.parameters(),  lr=LRC)#, weight_decay=0.01)\n",
    "policy_optimizer = opt.Adam(actor.parameters(), lr=LRA)\n",
    "\n",
    "MSE = nn.MSELoss()\n",
    "\n",
    "memory = replayBuffer(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through episodes\n",
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def subplot(R, P, Q, S):\n",
    "    r = list(zip(*R))\n",
    "    p = list(zip(*P))\n",
    "    q = list(zip(*Q))\n",
    "    s = list(zip(*S))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,15))\n",
    "\n",
    "    ax[0, 0].plot(list(r[1]), list(r[0]), 'r') #row=0, col=0\n",
    "    ax[1, 0].plot(list(p[1]), list(p[0]), 'b') #row=1, col=0\n",
    "    ax[0, 1].plot(list(q[1]), list(q[0]), 'g') #row=0, col=1\n",
    "    ax[1, 1].plot(list(s[1]), list(s[0]), 'k') #row=1, col=1\n",
    "    ax[0, 0].title.set_text('Reward')\n",
    "    ax[1, 0].title.set_text('Policy loss')\n",
    "    ax[0, 1].title.set_text('Q loss')\n",
    "    ax[1, 1].title.set_text('Max steps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Return: -3.4158162849236877\n",
      "Episode: 1 | Return: -3.8439777096469205\n",
      "Episode: 2 | Return: -3.4455796984633267\n",
      "Episode: 3 | Return: -3.759747633310704\n",
      "Episode: 4 | Return: -3.2154443576689804\n",
      "Episode: 5 | Return: -3.291989831941579\n",
      "Episode: 6 | Return: -3.341435116462127\n",
      "Episode: 7 | Return: -3.3071721080171423\n",
      "Episode: 8 | Return: -3.087278016259956\n",
      "Episode: 9 | Return: -3.633011145887119\n",
      "[    10 episode,     2000 total steps] average reward for past 10 iterations: 101.287\n",
      "Last model saved with reward: 220.23, at episode 2.\n",
      "Episode: 0 | Return: -2.7280337109924564\n",
      "Episode: 1 | Return: -3.0604352511040362\n",
      "Episode: 2 | Return: -3.3575922709820185\n",
      "Episode: 3 | Return: -3.7319664322829293\n",
      "Episode: 4 | Return: -3.429751459862408\n",
      "Episode: 5 | Return: -3.641576672559795\n",
      "Episode: 6 | Return: -3.431337009585664\n",
      "Episode: 7 | Return: -3.667503120435396\n",
      "Episode: 8 | Return: -3.5259362616329994\n",
      "Episode: 9 | Return: -3.4431727364407587\n",
      "[    10 episode,     4000 total steps] average reward for past 10 iterations: 78.404\n",
      "Last model saved with reward: 220.23, at episode 2.\n",
      "Episode: 0 | Return: -3.418538251965238\n",
      "Episode: 1 | Return: -2.750350153662765\n",
      "Episode: 2 | Return: -3.7207786703305725\n",
      "Episode: 3 | Return: -3.6090465959412406\n",
      "Episode: 4 | Return: -3.0490967431162965\n",
      "Episode: 5 | Return: -3.0715577559808454\n",
      "Episode: 6 | Return: -3.485899784565715\n",
      "Episode: 7 | Return: -3.651947881755384\n",
      "Episode: 8 | Return: -3.7389748036150867\n",
      "Episode: 9 | Return: -3.477809965873285\n",
      "[    10 episode,     6000 total steps] average reward for past 10 iterations: 74.545\n",
      "Last model saved with reward: 220.23, at episode 2.\n",
      "Episode: 0 | Return: 39.39572584713542\n",
      "Episode: 1 | Return: 37.40240392407109\n",
      "Episode: 2 | Return: 39.07651616094564\n",
      "Episode: 3 | Return: 37.34101325054794\n",
      "Episode: 4 | Return: 39.332407758000514\n",
      "Episode: 5 | Return: 37.08794702996664\n",
      "Episode: 6 | Return: 39.20585618609929\n",
      "Episode: 7 | Return: 37.09622882700676\n",
      "Episode: 8 | Return: 39.16077571583778\n",
      "Episode: 9 | Return: 39.38676129566163\n",
      "[    10 episode,     8000 total steps] average reward for past 10 iterations: 304.249\n",
      "Last model saved with reward: 345.36, at episode 38.\n",
      "Episode: 0 | Return: 25.963831487394195\n",
      "Episode: 1 | Return: 26.002532116325416\n",
      "Episode: 2 | Return: 26.48517596932627\n",
      "Episode: 3 | Return: 23.962789842907764\n",
      "Episode: 4 | Return: 24.0338122412022\n",
      "Episode: 5 | Return: 26.30791270243325\n",
      "Episode: 6 | Return: 24.301521566465876\n",
      "Episode: 7 | Return: 24.059754697803683\n",
      "Episode: 8 | Return: 23.940683747394036\n",
      "Episode: 9 | Return: 26.391040007162967\n",
      "[    10 episode,    10000 total steps] average reward for past 10 iterations: 304.890\n",
      "Last model saved with reward: 362.63, at episode 41.\n",
      "Episode: 0 | Return: 43.89235073801024\n",
      "Episode: 1 | Return: 42.89339734989305\n",
      "Episode: 2 | Return: 43.25663448108813\n",
      "Episode: 3 | Return: 42.746147969224694\n",
      "Episode: 4 | Return: 42.78337225877898\n",
      "Episode: 5 | Return: 43.46455828085288\n",
      "Episode: 6 | Return: 43.29999527520238\n",
      "Episode: 7 | Return: 43.22739773223285\n",
      "Episode: 8 | Return: 43.43842243778571\n",
      "Episode: 9 | Return: 42.9491850206097\n",
      "[    10 episode,    12000 total steps] average reward for past 10 iterations: 290.200\n",
      "Last model saved with reward: 362.63, at episode 41.\n",
      "Episode: 0 | Return: 57.272894087479195\n",
      "Episode: 1 | Return: 59.63904258214359\n",
      "Episode: 2 | Return: 92.33947059314421\n",
      "Episode: 3 | Return: 62.81211927597993\n",
      "Episode: 4 | Return: 91.71694441704187\n",
      "Episode: 5 | Return: 66.51952590785089\n",
      "Episode: 6 | Return: 91.35299407467159\n",
      "Episode: 7 | Return: 59.45229405518259\n",
      "Episode: 8 | Return: 61.98002704601578\n",
      "Episode: 9 | Return: 92.64030628950287\n",
      "[    10 episode,    14000 total steps] average reward for past 10 iterations: 317.924\n",
      "Last model saved with reward: 362.63, at episode 41.\n",
      "Episode: 0 | Return: 81.62231617877043\n",
      "Episode: 1 | Return: 57.42505333588596\n",
      "Episode: 2 | Return: 81.87445996799869\n",
      "Episode: 3 | Return: 60.0751664245082\n",
      "Episode: 4 | Return: 53.03516857492632\n",
      "Episode: 5 | Return: 57.23047096130504\n",
      "Episode: 6 | Return: 81.98109307116599\n",
      "Episode: 7 | Return: 57.480242541083406\n",
      "Episode: 8 | Return: 57.278463440515516\n",
      "Episode: 9 | Return: 84.20262906768859\n",
      "[    10 episode,    16000 total steps] average reward for past 10 iterations: 330.097\n",
      "Last model saved with reward: 362.63, at episode 41.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m policy_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mcritic(s_batch, actor(s_batch))\n\u001b[0;32m     66\u001b[0m policy_loss \u001b[39m=\u001b[39m policy_loss\u001b[39m.\u001b[39mmean()\n\u001b[1;32m---> 67\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     68\u001b[0m policy_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     70\u001b[0m \u001b[39m#soft update of the frozen target networks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\marco\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_reward = []\n",
    "plot_policy = []\n",
    "plot_q = []\n",
    "plot_steps = []\n",
    "\n",
    "\n",
    "best_reward = -np.inf\n",
    "saved_reward = -np.inf\n",
    "saved_ep = 0\n",
    "average_reward = 0\n",
    "global_step = 0\n",
    "#s = deepcopy(env.reset())\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    #print(episode)\n",
    "    s, _ = env.reset()\n",
    "    s = deepcopy(s)\n",
    "    #noise.reset()\n",
    "\n",
    "    ep_reward = 0.\n",
    "    ep_q_value = 0.\n",
    "    step=0\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "\n",
    "        #loss=0\n",
    "        global_step +=1\n",
    "        # epsilon -= epsilon_decay\n",
    "        #actor.eval()\n",
    "        a = actor.get_action(s)\n",
    "        #actor.train()\n",
    "\n",
    "        a += noise()*max(0, epsilon)\n",
    "        a = np.clip(a, -1., 1.)\n",
    "        s2, reward, terminal, info = env.step(a)\n",
    "\n",
    "\n",
    "        memory.add(s, a, reward, terminal,s2)\n",
    "\n",
    "        #keep adding experiences to the memory until there are at least minibatch size samples\n",
    "        \n",
    "        if memory.count() > buffer_start:\n",
    "            s_batch, a_batch, r_batch, t_batch, s2_batch = memory.sample(BATCH_SIZE)\n",
    "\n",
    "            s_batch = torch.FloatTensor(s_batch).to(device)\n",
    "            a_batch = torch.FloatTensor(a_batch).to(device)\n",
    "            r_batch = torch.FloatTensor(r_batch).unsqueeze(1).to(device)\n",
    "            t_batch = torch.FloatTensor(np.float32(t_batch)).unsqueeze(1).to(device)\n",
    "            s2_batch = torch.FloatTensor(s2_batch).to(device)\n",
    "            \n",
    "            \n",
    "            #compute loss for critic\n",
    "            a2_batch = target_actor(s2_batch)\n",
    "            target_q = target_critic(s2_batch, a2_batch) #detach to avoid updating target\n",
    "            y = r_batch + (1.0 - t_batch) * GAMMA * target_q.detach()\n",
    "            q = critic(s_batch, a_batch)\n",
    "            \n",
    "            q_optimizer.zero_grad()\n",
    "            q_loss = MSE(q, y) #detach to avoid updating target\n",
    "            q_loss.backward()\n",
    "            q_optimizer.step()\n",
    "            \n",
    "            #compute loss for actor\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss = -critic(s_batch, actor(s_batch))\n",
    "            policy_loss = policy_loss.mean()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "            #soft update of the frozen target networks\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - TAU) + param.data * TAU\n",
    "                )\n",
    "\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    target_param.data * (1.0 - TAU) + param.data * TAU\n",
    "                )\n",
    "\n",
    "        s = deepcopy(s2)\n",
    "        ep_reward += reward\n",
    "\n",
    "\n",
    "        #if terminal:\n",
    "        #    noise.reset()\n",
    "        #    break\n",
    "\n",
    "    try:\n",
    "        \n",
    "        plot_reward.append([ep_reward, episode+1])\n",
    "        plot_policy.append([policy_loss.data.cpu(), episode+1])\n",
    "        plot_q.append([q_loss.data.cpu(), episode+1])\n",
    "        plot_steps.append([step+1, episode+1])\n",
    "    except:\n",
    "        continue\n",
    "    average_reward += ep_reward\n",
    "    \n",
    "    if ep_reward > best_reward:\n",
    "        torch.save(actor.state_dict(), 'best_model_pendulum.pkl') #Save the actor model for future testing\n",
    "        best_reward = ep_reward\n",
    "        saved_reward = ep_reward\n",
    "        saved_ep = episode+1\n",
    "    \n",
    "    if (episode % PRINT_EVERY) == (PRINT_EVERY-1):    # print every print_every episodes\n",
    "        \n",
    "        for episode in range(10):\n",
    "                episode_reward = 0\n",
    "                done = False\n",
    "                state, _ = env.reset()\n",
    "\n",
    "                while not done:\n",
    "                    action = actor.get_action(state)\n",
    "                    state, reward, done, _  = env.step(action=action)\n",
    "    \n",
    "\n",
    "                    episode_reward += reward\n",
    "                print(f\"Episode: {episode} | Return: {episode_reward}\")\n",
    "        # subplot(plot_reward, plot_policy, plot_q, plot_steps)\n",
    "        # r = list(zip(*plot_reward))\n",
    "        # plt.plot(list(r[1]), list(r[0]), 'r') #row=0, col=0\n",
    "        # plt.show()\n",
    "        print('[%6d episode, %8d total steps] average reward for past {} iterations: %.3f'.format(PRINT_EVERY) %\n",
    "              (episode + 1, global_step, average_reward / PRINT_EVERY))\n",
    "        print(\"Last model saved with reward: {:.2f}, at episode {}.\".format(saved_reward, saved_ep))\n",
    "        average_reward = 0 #reset average reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7f6130942023c4e497e869b68f37e712da14dab2336769063882d8b350699c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
