# Raw Materials

## My Notes

* Usare Q e Q target e Experience Replay [here](https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c)
* [Divergence in Deep Q-Learning: Tips and Tricks](https://amanhussain.com/post/divergence-deep-q-learning/)

## Interesting Papers

* [Deep Reinforcement Learning that Matters](https://arxiv.org/pdf/1709.06560.pdf) -> DRL with hopper si basa molto su come scegliere parametri, ha spunti interessanti ma non so se Ã¨ fondamentale
  * [GitHub code of this paper](https://github.com/shubhlohiya/playing-atari-with-deep-RL/)
* [CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING](https://arxiv.org/pdf/1509.02971.pdf):
  * [Try To implement this paper on hopper v1](https://blog.paperspace.com/physics-control-tasks-with-deep-reinforcement-learning/)
    * [GitHub of the blog](https://github.com/antocapp/paperspace-ddpg-tutorial/blob/master/ddpg-pendulum-250.ipynb)
* [Improvements of 'playing Atari Games with Deep Reinforcement Learning' (14)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)